import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import datasets
import os

# 调整log等级，减少无关紧要的错误提示
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

# 手动导入数据集有两种，[mnist.npz]和[四个压缩包]，分别修改了[mnist.py]和[input_data.py]的数据路径
# 用input方式的代码如下
# from tensorflow.examples.tutorials.mnist import input_data
# mnist = input_data.read_data_sets('/Users/nicole/Documents/TensorFlow/MNIST_data/', one_hot=True)

(x, y), _ = datasets.mnist.load_data()

# x的区间为（0，255），但训练时使用（0，1）的数据更好，所以除以255
x = tf.convert_to_tensor(x, dtype=tf.float32) / 255.
# y在后面进行onehot的处理
y = tf.convert_to_tensor(y, dtype=tf.int32)

# 为了查看x，y的相关值，结果如下
# (60000, 28, 28) (60000,) <dtype: 'float32'> <dtype: 'int32'>
# tf.Tensor(0.0, shape=(), dtype=float32) tf.Tensor(1.0, shape=(), dtype=float32)
# tf.Tensor(0, shape=(), dtype=int32) tf.Tensor(9, shape=(), dtype=int32)
print(x.shape, y.shape, x.dtype, y.dtype)
print(tf.reduce_min(x), tf.reduce_max(x))
print(tf.reduce_min(y), tf.reduce_max(y))

# 由于数据集有60K，数据太多，所以通过设定batch分次取数据，一次128个
train_db = tf.data.Dataset.from_tensor_slices((x, y)).batch(128)
train_iter = iter(train_db)
sample = next(train_iter)

# 查看一个batch的值，结果如下（分别为x，y的值）
# batch: (128, 28, 28) (128,)
print('batch:', sample[0].shape, sample[1].shape)

# 定义每个需要进行优化的变量（这里的tf.Variable是必须的，如果没有回导致求导后的值为None）
# 初始方差值默认为[1]，但对于本次训练，方差[1]过大会发生梯度爆炸，因此追加[stddev=0.1]使方差初始值为0.1
# 梯度爆炸现象：[loss：nan]
w1 = tf.Variable(tf.random.truncated_normal([784, 256], stddev=0.1))
b1 = tf.Variable(tf.zeros([256]))
w2 = tf.Variable(tf.random.truncated_normal([256, 128], stddev=0.1))
b2 = tf.Variable(tf.zeros([128]))
w3 = tf.Variable(tf.random.truncated_normal([128, 10], stddev=0.1))
b3 = tf.Variable(tf.zeros([10]))

# 通常使用的学习率 （1e-3 = 0.001）
lr = 1e-3

# epoch为对数据集整体进行循环训练，为了使结果更加逼近（这里为循环10次）
for epoch in range(10):

    # step 为每一个batch为一个step，可以阶段性的查看结果
    for step, (x, y) in enumerate(train_db):
        
        # reshape时，[-1]相当于未知数，由程序自己算出，所以一个式子中只能出现一个[-1]
        x = tf.reshape(x, [-1, 28*28])
        
        # 对待优化变量进行求导
        with tf.GradientTape() as tape:
            h1 = x@w1 + tf.broadcast_to(b1, [x.shape[0], 256])
            h1 = tf.nn.relu(h1)
            h2 = h1@w2 + tf.broadcast_to(b2, [h1.shape[0], 128])
            h2 = tf.nn.relu(h2)
            out = h2@w3 + tf.broadcast_to(b3, [h2.shape[0], 10])

            y_onehot = tf.one_hot(y, depth=10)

            # 求误差的平均方差  mse = mean(sum(y - out)^2)
            loss = tf.square(y_onehot - out)
            loss = tf.reduce_mean(loss)
        
        # 将求导后的值赋予grads
        grads = tape.gradient(loss, [w1, b1, w2, b2, w3, b3])
        
        # 此处的数学公式为 W1 = W1 - lr * grads[0]
        # 但由于上述公式的返回值为tensor类型，会导致第二次优化时的grads为None，因此需用assign_sub()的函数，对其进行原地更新（保持类型不变）
        w1.assign_sub(lr * grads[0])
        b1.assign_sub(lr * grads[1])
        w2.assign_sub(lr * grads[2])
        b2.assign_sub(lr * grads[3])
        w3.assign_sub(lr * grads[4])
        b3.assign_sub(lr * grads[5])

        if step % 100 == 0:
            print(epoch, step, 'loss', float(loss))



# result
# epoch的循环对结果的优化很明显[0.18043871223926544] →[0.08972378075122833]
========================================================
0 0 loss 0.27372971177101135
0 100 loss 0.19032660126686096
0 200 loss 0.19035513699054718
0 300 loss 0.16698487102985382
0 400 loss 0.18043871223926544
1 0 loss 0.1659463346004486
1 100 loss 0.1515551060438156
1 200 loss 0.1547560691833496
1 300 loss 0.1404707431793213
1 400 loss 0.15142884850502014
2 0 loss 0.141214519739151
2 100 loss 0.1316833198070526
2 200 loss 0.133863627910614
2 300 loss 0.12461726367473602
2 400 loss 0.13379177451133728
3 0 loss 0.12545685470104218
3 100 loss 0.1187557727098465
3 200 loss 0.11991152912378311
3 300 loss 0.11389361321926117
3 400 loss 0.121856190264225
4 0 loss 0.11441247165203094
4 100 loss 0.10957694053649902
4 200 loss 0.1098448783159256
4 300 loss 0.10612519830465317
4 400 loss 0.11310248076915741
5 0 loss 0.10622112452983856
5 100 loss 0.10263247787952423
5 200 loss 0.10224372148513794
5 300 loss 0.10021328926086426
5 400 loss 0.10635773837566376
6 0 loss 0.09980116784572601
6 100 loss 0.09716400504112244
6 200 loss 0.09625610709190369
6 300 loss 0.09542585909366608
6 400 loss 0.10099264234304428
7 0 loss 0.09453549236059189
7 100 loss 0.09274760633707047
7 200 loss 0.09140942990779877
7 300 loss 0.09145508706569672
7 400 loss 0.09658787399530411
8 0 loss 0.0901658907532692
8 100 loss 0.08908423036336899
8 200 loss 0.08736599236726761
8 300 loss 0.08806339651346207
8 400 loss 0.09289859235286713
9 0 loss 0.08646990358829498
9 100 loss 0.08596999943256378
9 200 loss 0.08395081013441086
9 300 loss 0.08511843532323837
9 400 loss 0.08972378075122833

Process finished with exit code 0
